{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ6p8pcJQKPw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MTG/da-tacos.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('da-tacos')"
      ],
      "metadata": {
        "id": "R8V5L47rQQ-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Shwl5-bnQY4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running directly from Colab UI\n",
        "os.makedirs('/content/coveranalysis_single_files_output', exist_ok=True)\n",
        "!python3 download_da-tacos.py --dataset coveranalysis --type single_files --source gdrive --outputdir /content/coveranalysis_single_files_output\n",
        "!unzip /content/coveranalysis_single_files_output/da-tacos_coveranalysis_subset_single_files.zip -d /content/coveranalysis_single_files_output\n"
      ],
      "metadata": {
        "id": "x8i821lPQcut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running through my gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/da-tacos_coveranalysis_subset_single_files.zip -d /content/drive/MyDrive/datacos"
      ],
      "metadata": {
        "id": "oqciJAUTRuk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import h5py\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "v6si1C_pRbow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "bfYU2RplSt0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Class - Creating pairs with labels Y=0 and Y=1 and returns a compined tensor\n",
        "\n",
        "import deepdish as dd\n",
        "\n",
        "class DATACOSDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading in the pairs from the directory structure.\n",
        "        Each subfolder contains 2 cover-related songs, and you create pairs with labels.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.pairs = []  # Will store (song1, song2, label)\n",
        "        self.subfolders = [f.path for f in os.scandir(data_dir) if f.is_dir()]\n",
        "\n",
        "        # Generate pairs with label Y=0 (cover-related)\n",
        "        for folder in self.subfolders:\n",
        "            songs = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.h5')]\n",
        "            if len(songs) == 2:  # Assumption: each folder contains exactly 2 songs\n",
        "                cover, original = songs\n",
        "                self.pairs.append((cover, original, 0))  # Y=0 for similar pairs (cover-related)\n",
        "\n",
        "        # Generate non-cover pairs with label Y=1\n",
        "        all_songs_by_folder = {folder: [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.h5')] for folder in self.subfolders}\n",
        "\n",
        "        for _ in range(len(self.pairs)):  # Generate the same number of non-cover pairs\n",
        "            # Randomly sample two different subfolders\n",
        "            folder1, folder2 = random.sample(self.subfolders, 2)  # Ensure two different subfolders\n",
        "            song1 = random.choice(all_songs_by_folder[folder1])\n",
        "            song2 = random.choice(all_songs_by_folder[folder2])\n",
        "\n",
        "            self.pairs.append((song1, song2, 1))  # Y=1 for non-similar pairs (not cover-related)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        song1_path, song2_path, label = self.pairs[idx]\n",
        "\n",
        "        # Load the .h5 files\n",
        "        song1_data = self.load_h5(song1_path)\n",
        "        song2_data = self.load_h5(song2_path)\n",
        "\n",
        "        if self.transform:\n",
        "            song1_data = self.transform(song1_data)\n",
        "            song2_data = self.transform(song2_data)\n",
        "\n",
        "        return song1_data, song2_data, torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "    def load_h5(self, file_path):\n",
        "        \"\"\"\n",
        "        Helper function to load the .h5 file and return a tensor combining the features.\n",
        "        \"\"\"\n",
        "        # Load the .h5 file using deepdish\n",
        "        data = dd.io.load(file_path)\n",
        "\n",
        "        # Extract all requested features\n",
        "        chroma_cens_data = data.get('chroma_cens', None)\n",
        "        crema_data = data.get('crema', None)\n",
        "        hpcp_data = data.get('hpcp', None)\n",
        "        key_extractor_data = data.get('key_extractor', None)\n",
        "        madmom_features_data = data.get('madmom_features', None)\n",
        "        mfcc_htk_data = data.get('mfcc_htk', None)\n",
        "        tags_data = data.get('tags', None)\n",
        "        label_data = data.get('label', None)\n",
        "\n",
        "        feature_list = []\n",
        "\n",
        "        # Append the features (ensure they are tensors)\n",
        "        if chroma_cens_data is not None:\n",
        "            chroma_cens_data = torch.tensor(chroma_cens_data, dtype=torch.float)\n",
        "            if chroma_cens_data.dim() == 1:\n",
        "                chroma_cens_data = chroma_cens_data.unsqueeze(-1)  # Reshape to 2D if necessary\n",
        "            feature_list.append(chroma_cens_data)\n",
        "        if crema_data is not None:\n",
        "            crema_data = torch.tensor(crema_data, dtype=torch.float)\n",
        "            if crema_data.dim() == 1:\n",
        "                crema_data = crema_data.unsqueeze(-1)  # Reshape to 2D if necessary\n",
        "            feature_list.append(crema_data)\n",
        "        if hpcp_data is not None:\n",
        "            hpcp_data = torch.tensor(hpcp_data, dtype=torch.float)\n",
        "            if hpcp_data.dim() == 1:\n",
        "                hpcp_data = hpcp_data.unsqueeze(-1)  # Reshape to 2D if necessary\n",
        "            feature_list.append(hpcp_data)\n",
        "\n",
        "        # Handle the key_extractor_data (if it's a dictionary, extract relevant data)\n",
        "        if key_extractor_data is not None:\n",
        "            if isinstance(key_extractor_data, dict):\n",
        "                key_extractor_values = key_extractor_data.get('tonnetz', None)  # Adjust based on actual structure\n",
        "                if key_extractor_values is not None:\n",
        "                    key_extractor_values = torch.tensor(key_extractor_values, dtype=torch.float)\n",
        "                    if key_extractor_values.dim() == 1:\n",
        "                        key_extractor_values = key_extractor_values.unsqueeze(-1)\n",
        "                    feature_list.append(key_extractor_values)\n",
        "            else:\n",
        "                key_extractor_data = torch.tensor(key_extractor_data, dtype=torch.float)\n",
        "                if key_extractor_data.dim() == 1:\n",
        "                    key_extractor_data = key_extractor_data.unsqueeze(-1)\n",
        "                feature_list.append(key_extractor_data)\n",
        "\n",
        "        # Handle madmom_features_data (if it's a dictionary, extract relevant data)\n",
        "        if madmom_features_data is not None:\n",
        "            if isinstance(madmom_features_data, dict):\n",
        "                madmom_values = madmom_features_data.get('tempo', None)  # Adjust key based on actual structure\n",
        "                if madmom_values is not None:\n",
        "                    madmom_values = torch.tensor(madmom_values, dtype=torch.float)\n",
        "                    if madmom_values.dim() == 1:\n",
        "                        madmom_values = madmom_values.unsqueeze(-1)\n",
        "                    feature_list.append(madmom_values)\n",
        "            else:\n",
        "                madmom_features_data = torch.tensor(madmom_features_data, dtype=torch.float)\n",
        "                if madmom_features_data.dim() == 1:\n",
        "                    madmom_features_data = madmom_features_data.unsqueeze(-1)\n",
        "                feature_list.append(madmom_features_data)\n",
        "\n",
        "        if mfcc_htk_data is not None:\n",
        "            mfcc_htk_data = torch.tensor(mfcc_htk_data, dtype=torch.float)\n",
        "            if mfcc_htk_data.dim() == 1:\n",
        "                mfcc_htk_data = mfcc_htk_data.unsqueeze(-1)  # Reshape to 2D if necessary\n",
        "            feature_list.append(mfcc_htk_data)\n",
        "\n",
        "        # Handle tags_data (check if it's a string or list of strings)\n",
        "        if tags_data is not None:\n",
        "            if isinstance(tags_data, str):\n",
        "                tag_map = {'tag1': 0, 'tag2': 1, 'tag3': 2}  # Example: map your tags to integers\n",
        "                tag_value = tag_map.get(tags_data, -1)  # Use -1 for unknown tags\n",
        "                tag_tensor = torch.tensor([tag_value], dtype=torch.float).unsqueeze(-1)\n",
        "                feature_list.append(tag_tensor)\n",
        "            elif isinstance(tags_data, list):  # If it's a list of tags (strings)\n",
        "                tag_map = {'tag1': 0, 'tag2': 1, 'tag3': 2}  # Example: map your tags to integers\n",
        "                tag_values = [tag_map.get(tag, -1) for tag in tags_data]  # Use -1 for unknown tags\n",
        "                tag_tensor = torch.tensor(tag_values, dtype=torch.float).unsqueeze(-1)\n",
        "                feature_list.append(tag_tensor)\n",
        "\n",
        "        # Pad features to the same size if necessary\n",
        "        feature_lengths = [f.shape[0] for f in feature_list]  # Get the lengths of each feature\n",
        "        max_length = max(feature_lengths)  # Find the maximum length\n",
        "\n",
        "        # Pad the features to have the same length\n",
        "        for i, feature in enumerate(feature_list):\n",
        "            if feature.shape[0] < max_length:\n",
        "                pad_size = max_length - feature.shape[0]\n",
        "                feature_list[i] = torch.nn.functional.pad(feature, (0, 0, 0, pad_size))\n",
        "\n",
        "        # Stack all features together\n",
        "        combined_features = torch.cat(feature_list, dim=-1)\n",
        "\n",
        "        return combined_features"
      ],
      "metadata": {
        "id": "FCxhd0A2S5fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of DatacosClass usage\n",
        "dataset = DATACOSDataset(data_dir='/content/drive/MyDrive/datacos/da-tacos_coveranalysis_subset_single_files')\n",
        "\n",
        "for i in range(4999, 5001):\n",
        "  # Print the first 5 pairs\n",
        "    song1_data, song2_data, label = dataset[i]\n",
        "    print(f\"Pair {i+1}:\")\n",
        "    print(f\"Song 1 Data: {song1_data.shape}, Song 2 Data: {song2_data.shape}, Label: {label}\")"
      ],
      "metadata": {
        "id": "QpGlw0G9WHA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_1 = [dataset[i] for i in range(4000)]  # First 4000 pairs for training\n",
        "train_data_2 = [dataset[i] for i in range(5000, 9000)]  # Next 4000 pairs for training (5000 to 8999)\n",
        "\n",
        "val_data_1 = [dataset[i] for i in range(4000, 5000)]  # 1000 pairs for validation (4000 to 4999)\n",
        "val_data_2 = [dataset[i] for i in range(9000, 10000)]  # 1000 pairs for validation (9000 to 9999)\n",
        "\n",
        "# Combine the training and validation parts\n",
        "train_data = train_data_1 + train_data_2  # Final training data (4000 + 4000)\n",
        "val_data = val_data_1 + val_data_2  # Final validation data (1000 + 1000)\n",
        "\n",
        "# Shuffle both the training and validation datasets\n",
        "random.shuffle(train_data)\n",
        "random.shuffle(val_data)"
      ],
      "metadata": {
        "id": "V9YL7V-HdZeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking in a small set of the dataset\n",
        "\n",
        "train_data_1 = [dataset[i] for i in range(2)]\n",
        "train_data_2 = [dataset[i] for i in range(5, 6)]\n",
        "\n",
        "val_data_1 = [dataset[i] for i in range(4, 5)]\n",
        "val_data_2 = [dataset[i] for i in range(9, 10)]\n",
        "\n",
        "# Combine the training and validation parts\n",
        "train_data = train_data_1 + train_data_2\n",
        "val_data = val_data_1 + val_data_2\n",
        "\n",
        "# Shuffle both the training and validation datasets\n",
        "random.shuffle(train_data)\n",
        "random.shuffle(val_data)"
      ],
      "metadata": {
        "id": "0awx8if9d2ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the memory usage of individual samples\n",
        "song1_data, song2_data, label = dataset[0]\n",
        "print(f\"Memory usage of song1_data: {song1_data.element_size() * song1_data.nelement() / (1024 ** 3)} GB\")\n",
        "print(f\"Memory usage of song2_data: {song2_data.element_size() * song2_data.nelement() / (1024 ** 3)} GB\")"
      ],
      "metadata": {
        "id": "zo6j_vv-ddxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader objects for both train and validation sets\n",
        "trainloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "validloader = DataLoader(val_data, batch_size=128, shuffle=True)\n",
        "\n",
        "# Check the size of trainloader and validloader\n",
        "print(f\"Trainloader size: {len(trainloader)} batches\")\n",
        "print(f\"Validloader size: {len(validloader)} batches\")"
      ],
      "metadata": {
        "id": "K9dXPk9Wdk-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Architecture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class DrLIM(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Conv2d(in_channels = 1, out_channels = 15, kernel_size = 6, padding = 0, stride = 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.subsampling = nn.MaxPool2d(kernel_size = 3, stride = 3)\n",
        "        self.layer2 = nn.Conv2d(in_channels = 15, out_channels = 30, kernel_size = 9, padding = 0, stride = 1)\n",
        "        # self.relu = nn.ReLU(),\n",
        "        self.fc = nn.Linear(15, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.subsampling(x)\n",
        "        x = self.layer2(x)\n",
        "        x = x.reshape(-1, 2, 15)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iL9hP2KFebfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CL_Loss(x1, x2, Y, m = 1):\n",
        "    Euclidean_norm = torch.sqrt((x1 - x2)**2) # Euclidean Distance\n",
        "    return torch.mean((1-Y).reshape(-1, 1, 1) * 1/2 * Euclidean_norm**2 + Y.reshape(-1, 1, 1) * 1/2 * torch.maximum(torch.Tensor([0]).to(device), m - Euclidean_norm)**2)"
      ],
      "metadata": {
        "id": "zE_fVhg5ehcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "o6tyzt88esUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training parameters\n",
        "\n",
        "epochs = 200 # Iteration Number\n",
        "cnt = 0      # early stopping count\n",
        "\n",
        "model = DrLIM().to(device)\n",
        "# criterion = CL_Loss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001) # Adam Optimizer\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model).to(device)\n",
        "\n",
        "# save train and val Loss\n",
        "train_loss = torch.zeros(epochs)\n",
        "val_loss = torch.zeros(epochs)\n",
        "\n",
        "# save train and val Acc\n",
        "train_acc = torch.zeros(epochs)\n",
        "val_acc = torch.zeros(epochs)\n",
        "\n",
        "# initial loss value is inf.\n",
        "valid_loss_min = np.Inf\n",
        "valid_acc_max = 0"
      ],
      "metadata": {
        "id": "OWDQ0nh3e9Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the Model\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train() # train mode\n",
        "\n",
        "    for x1, x2, y in tqdm(trainloader):\n",
        "        y = torch.tensor(y, dtype = torch.float64).to(device)\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "        optimizer.zero_grad() # optimizer initialization\n",
        "\n",
        "        Gw_x1 = model(x1) # model output of x1\n",
        "        Gw_x2 = model(x2) # model output of x2\n",
        "\n",
        "        # Calculate accuracy\n",
        "        loss = CL_Loss(Gw_x1, Gw_x2, y)\n",
        "        loss.backward() # backward\n",
        "        optimizer.step() # optimizer step\n",
        "        train_loss[epoch] += loss.item() # loss\n",
        "\n",
        "        ans = torch.tensor((Gw_x1[:, 0] < Gw_x1[:, 1]) != (Gw_x2[:, 0] < Gw_x2[:, 1]), dtype=torch.float64) # Similar to Contrastive Leaning Loss\n",
        "        equals = ans == y.reshape(ans.shape)   # calculate acc\n",
        "        train_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item()  # mean\n",
        "\n",
        "    # AVG Loss\n",
        "    train_loss[epoch] /= len(trainloader)\n",
        "    train_acc[epoch] /= len(trainloader)\n",
        "\n",
        "\n",
        "    # valid ,\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x1, x2, y in tqdm(validloader):\n",
        "            y = torch.tensor(y, dtype = torch.float64).to(device)\n",
        "            x1, x2 = x1.to(device), x2.to(device)\n",
        "\n",
        "            Gw_x1 = model(x1)\n",
        "            Gw_x2 = model(x2)\n",
        "\n",
        "            loss = CL_Loss(Gw_x1, Gw_x2, y)\n",
        "            val_loss[epoch] += loss.item() # Loss\n",
        "\n",
        "\n",
        "            ans = torch.tensor((Gw_x1[:, 0] < Gw_x1[:, 1]) != (Gw_x2[:, 0] < Gw_x2[:, 1]), dtype=torch.float64)\n",
        "            equals = ans == y.reshape(ans.shape)\n",
        "            val_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item()  # mean\n",
        "\n",
        "    # validation Loss and accuracy\n",
        "    val_loss[epoch] /= len(validloader)\n",
        "    val_acc[epoch] /= len(validloader)\n",
        "\n",
        "    # print loss and accuracy\n",
        "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "          f\"Train loss: {train_loss[epoch]:.3f}.. \"\n",
        "          f\"Train acc: {train_acc[epoch]:.3f}.. \"\n",
        "          f\"val loss: {val_loss[epoch]:.3f}.. \"\n",
        "          f\"val accuracy: {val_acc[epoch]:.3f}\")\n",
        "\n",
        "    if val_acc[epoch] >= valid_acc_max:\n",
        "        print('Validation acc increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_acc_max,\n",
        "        val_acc[epoch]))\n",
        "        torch.save(model.module.state_dict(), 'DrLIM.pt')\n",
        "        valid_acc_max = val_acc[epoch]\n",
        "\n",
        "        # Early stopping\n",
        "        cnt = 0\n",
        "\n",
        "    # Early stopping and Loss\n",
        "    if cnt >= 10:\n",
        "        print(\"Early Stopping\")\n",
        "        break\n",
        "\n",
        "    cnt+=1 #Loss\n"
      ],
      "metadata": {
        "id": "j1u20z-5fHj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Potential Transforms - 1.FFT-Downsampling\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'tensor' is our DatasetClass output\n",
        "tensor = np.random.random((20890, 41760))\n",
        "\n",
        "# Apply FFT to get frequency representation\n",
        "fft_tensor = np.fft.fft2(tensor)\n",
        "\n",
        "# Filter out selected frequencies (e.g., keep low frequencies only or whatever wroks with our given task)\n",
        "# For instance, zero out all but the lowest frequencies\n",
        "fft_tensor[100:, :] = 0  # Zero out high frequencies along one dimension\n",
        "fft_tensor[:, 100:] = 0  # Zero out high frequencies along the other dimension\n",
        "\n",
        "# Apply inverse FFT to get the downsampled tensor in spatial domain\n",
        "downsampled_tensor = np.fft.ifft2(fft_tensor)\n"
      ],
      "metadata": {
        "id": "mCk06-rtrMxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Potential Transforms - 2.PCA\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `tensor` is your original 2D tensor (e.g., [20890, 41760])\n",
        "tensor = np.random.random((20890, 41760))\n",
        "\n",
        "# Apply FFT to the tensor to get the frequency domain representation\n",
        "fft_tensor = np.fft.fft2(tensor)\n",
        "\n",
        "# Flatten the FFT result to apply PCA\n",
        "fft_flattened = fft_tensor.flatten().reshape(-1, 1)\n",
        "\n",
        "# Perform PCA to reduce dimensionality (e.g., keeping top 10 components)\n",
        "pca = PCA(n_components=10)\n",
        "pca_result = pca.fit_transform(fft_flattened)\n",
        "\n",
        "# Reconstruct the downsampled tensor using the inverse PCA transformation\n",
        "downsampled_tensor = pca.inverse_transform(pca_result)"
      ],
      "metadata": {
        "id": "mp_OJ8HWrPQf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}